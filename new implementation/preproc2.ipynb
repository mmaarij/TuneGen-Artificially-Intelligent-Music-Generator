{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c43f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 01:07:49.089951: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-17 01:07:51.846203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/maarij/anaconda3/envs/tf/lib/\n",
      "2023-02-17 01:07:51.846840: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/maarij/anaconda3/envs/tf/lib/\n",
      "2023-02-17 01:07:51.846865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from midi_to_dataframe import NoteMapper, MidiReader, MidiWriter\n",
    "import IPython\n",
    "from IPython.display import Image, IFrame\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import music21\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec629d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_mapping_config_path = \"./config/map-to-group.json\"\n",
    "note_mapper = NoteMapper(note_mapping_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c0a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = MidiReader(note_mapper)\n",
    "\n",
    "# getting midi files\n",
    "filepath = \"./datasets/dataset_pop/\"\n",
    "MidiDataDF = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(filepath):\n",
    "    if filename.endswith(\".midi\"):\n",
    "        \n",
    "        # create file path\n",
    "        count += 1\n",
    "        #print(count, filename, end = \" \")\n",
    "        fullFilePath = filepath+filename\n",
    "\n",
    "        # read file as dataframe\n",
    "        tempDF = reader.convert_to_dataframe(fullFilePath)\n",
    "        #print(tempDF.shape[0])\n",
    "        MidiDataDF = MidiDataDF.append(tempDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba31b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(MidiDataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e33709",
   "metadata": {},
   "outputs": [],
   "source": [
    "NotesDataDF = MidiDataDF[[\"notes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e65e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the notes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(NotesDataDF[\"notes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "336e6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating input sequences\n",
    "sequences = []\n",
    "for note in NotesDataDF['notes']:\n",
    "    sequence = tokenizer.texts_to_sequences([note])[0]\n",
    "    for i in range(1, len(sequence)):\n",
    "        n_gram_sequence = sequence[:i+1]\n",
    "        sequences.append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93535620",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Padding sequences\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m max_sequence_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m padded_sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pad_sequences(sequences, maxlen\u001b[38;5;241m=\u001b[39mmax_sequence_len, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# Padding sequences\n",
    "max_sequence_len = max([len(sequence) for sequence in sequences])\n",
    "padded_sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83274b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating predictors and target\n",
    "predictors, target = padded_sequences[:,:-1], padded_sequences[:,-1]\n",
    "target = to_categorical(target, num_classes=len(tokenizer.word_index)+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
